#!/usr/bin/env python
"""
NAME: {{name}}
=========

DESCRIPTION
===========

INSTALLATION
============

USAGE
=====

VERSION HISTORY
===============

{{version}}   {{date}}    Initial version.

LICENCE
=======
2016, copyright {{author}} ({{email}}), {{url}}

template version: 1.5 (2016/09/09)
"""
from timeit import default_timer as timer
from multiprocessing import Pool
from ConfigParser import SafeConfigParser
from signal import signal, SIGPIPE, SIG_DFL
import sys
import os
import os.path
import argparse
import csv
import collections
import gzip
import bz2
import zipfile
import time
import pandas as pd  ## non standard library to be imported

# When piping stdout into head python raises an exception
# Ignore SIG_PIPE and don't throw exceptions on it...
# (http://docs.python.org/library/signal.html)
signal(SIGPIPE, SIG_DFL)

__version__ = '{{version}}'
__date__ = '{{date}}'
__email__ = '{{email}}'
__author__ = '{{author}}'


def parse_cmdline():
    """ Parse command-line args. """
    ## parse cmd-line -----------------------------------------------------------
    description = 'Read delimited file.'
    version = 'version %s, date %s' % (__version__, __date__)
    epilog = 'Copyright %s (%s)' % (__author__, __email__)

    parser = argparse.ArgumentParser(description=description, epilog=epilog)

    parser.add_argument('--version',
                        action='version',
                        version='%s' % (version))

    parser.add_argument(
        'str_file',
        metavar='FILE',
        help=
        'Delimited file. [if set to "-" or "stdin" reads from standard in]')
    parser.add_argument('-a',
                        '--header',
                        dest='header_exists',
                        action='store_true',
                        default=False,
                        help='Header in File. [default: False]')
    parser.add_argument('-d',
                        '--delimiter',
                        metavar='STRING',
                        dest='delimiter_str',
                        default='\t',
                        help='Delimiter used in file.  [default: "tab"]')
    parser.add_argument('-f',
                        '--field',
                        metavar='INT',
                        type=int,
                        dest='field_number',
                        default=1,
                        help='Field / Column in file to use.  [default: 1]')
    parser.add_argument('-o',
                        '--out',
                        metavar='STRING',
                        dest='outfile_name',
                        default=None,
                        help='Out-file. [default: "stdout"]')

    # PANDAS
    parser.add_argument('-l',
                        '--labels',
                        dest='label_number',
                        metavar="INT",
                        type=int,
                        default=None,
                        help='Column number to use as labels. [default: None]')
    # FOR CONFIG-FILE PARSING
    parser.add_argument('--config',
                        dest='configfile_name',
                        metavar='CONFIG-FILE',
                        default='config.ini',
                        help='Config-file to read. [default: config.ini]')

    group1 = parser.add_argument_group('Threading',
                                       'Multithreading arguments:')

    group1.add_argument(
        '-p',
        '--processes',
        metavar='INT',
        type=int,
        dest='process_number',
        default=1,
        help=
        'Number of sub-processes (workers) to use.'+\
        ' It is only logical to not give more processes'+\
        ' than cpus/cores are available. [default: 1]')
    group1.add_argument(
        '-t',
        '--time',
        action='store_true',
        dest='show_runtime',
        default=False,
        help='Time the runtime and print to stderr. [default: False]')

    args = parser.parse_args()
    return args, parser


def load_file(filename):
    """ LOADING FILES """
    if filename in ['-', 'stdin']:
        filehandle = sys.stdin
    elif filename.split('.')[-1] == 'gz':
        filehandle = gzip.open(filename)
    elif filename.split('.')[-1] == 'bz2':
        filehandle = bz2.BZFile(filename)
    elif filename.split('.')[-1] == 'zip':
        filehandle = zipfile.Zipfile(filename)
    else:
        filehandle = open(filename)
    return filehandle


def my_func(args):
    """
    THIS IS THE ACCTUAL WORKFUNCTION THAT HAS TO BE EXECUTED MULTPLE TIMES.
    This funion will be distributed to the cores requested.
    # do stuff
    res = ...
    return (args, res)
    """
    # Do stuff and create result
    # EXAMPLE: Here we add up arg1 and arg2 and wait a bit.
    res = args[0] + args[1]
    time.sleep(0.2)
    return (args, res)


def main():
    """ The main funtion. """
    args, parser = parse_cmdline()

    # get field number to use in infile
    field_number = args.field_number - 1
    if field_number < 0:
        parser.error('Field -f has to be an integer > 0. EXIT.')

    fileobj = load_file(args.str_file)

    # create outfile object
    if not args.outfile_name:
        outfileobj = sys.stdout
    elif args.outfile_name in ['-', 'stdout']:
        outfileobj = sys.stdout
    elif args.outfile_name.split('.')[-1] == 'gz':
        outfileobj = gzip.open(args.outfile_name, 'wb')
    else:
        outfileobj = open(args.outfile_name, 'w')

    # -------------------------------------------------------
    # USING a config parser
    # Read config file if it exists
    # e.g.
    # [Classes]
    # c1 = 1
    # c2 = 3
    param_dict = {}
    if os.path.isfile(args.configfile_name):
        config_parser_obj = SafeConfigParser()
        config_parser_obj.read(args.configfile_name)
        for section_name in config_parser_obj.sections():
            for name, value in config_parser_obj.items(section_name):
                param_dict[name] = value
    else:
        sys.stderr.write('Could not load configfile: %s. EXIT.' % (args.configfile_name))
        sys.exit()
    # -------------------------------------------------------

    # ------------------------------------------------------
    # PANDAS approach
    # Check labels
    if args.label_number:
        if args.label_number <= 0:
            parser.error('Label column number has to be > 0. EXIT.')
        label_number = args.label_number - 1
    else:
        label_number = False
    data_frame = pd.read_csv(fileobj,
                             sep=args.delimiter_str,
                             index_col=label_number)
    # ------------------------------------------------------

    # ------------------------------------------------------
    #  THREADING
    # ------------------------------------------------------
    # get number of subprocesses to use
    process_number = args.process_number
    if process_number < 1:
        parser.error('-p has to be > 0: EXIT.')

    # FILL ARRAY WITH PARAMETER SETS TO PROCESS
    #
    # this array contains the total amount of jobs that has to be run
    job_list = []

    # e.g. read tasks from file
    # delimited file handler
    csv_reader_obj = csv.reader(fileobj, delimiter=args.delimiter_str)
    if args.header_exists:
        header_list = csv_reader_obj.next()
    for row in csv_reader_obj:
        # EXAMPLE: 2 parameters = first two cols for my_func
        # EXAMPLE: TWO INTEGERS TO ADD UP
        try:
            job_list.append((int(row[0]), int(row[1])))
        except:
            parser.error('Need 2 integers in column 1 and 2 to add up.')
    fileobj.close()

    # For timing
    start_time = timer()  # very crude
    # create pool of workers ---------------------
    pool = Pool(processes=process_number)

    # "chunksize"" usually only makes a noticable performance
    # difference for very large iterables
    # Here I set it to one to get the progress bar working nicly
    # Otherwise it will not give me the correct number of processes left
    # but chunksize number.
    chunksize = 1

    result_list = pool.map_async(my_func, job_list, chunksize=chunksize)
    pool.close()  # No more work

    jobs_total = len(job_list)
    # Progress bar
    #==============================
    # This can be changed to make progressbar bigger or smaller
    progress_bar_length = 50
    #==============================
    while not result_list.ready():
        num_not_done = result_list._number_left
        num_done = jobs_total - num_not_done
        num_bar_done = num_done * progress_bar_length / jobs_total
        bar_str = ('=' * num_bar_done).ljust(progress_bar_length)
        percent = int(num_done * 100 / jobs_total)
        sys.stderr.write("JOBS (%s): [%s] (%s) %s%%\r" % (str(num_not_done).rjust(len(str(jobs_total))),
                                                          bar_str,
                                                          str(num_done).rjust(len(str(jobs_total))),
                                                          str(percent).rjust(3)))
 
        sys.stderr.flush()
        time.sleep(0.1)  # wait a bit: here we test all .1 secs
    # Finish the progress bar
    bar_str = '=' * progress_bar_length
    sys.stderr.write("JOBS (%s): [%s] (%i) 100%%\n" % ('0'.rjust(len(str(jobs_total))),
                                                       bar_str,
                                                       jobs_total))
    result_list = result_list.get()
    # --------------------------------------------

    end_time = timer()
    if args.show_runtime:
        sys.stderr.write('RUNTIME(s): %.4f\n' % (end_time - start_time))

    # Do stuff with the results
    for res in result_list:
        outfileobj.write('%s,%s:\t%s\n' % (str(res[0][0]),
                                           str(res[0][1]),
                                           str(res[1])))
    # ------------------------------------------------------
    outfileobj.close()
    return


if __name__ == '__main__':
    sys.exit(main())
